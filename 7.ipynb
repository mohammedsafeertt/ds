{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wSG1ss6ri5-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "class Apriori:\n",
        "    def __init__(self, min_support=0.5, min_confidence=0.7):\n",
        "        \"\"\"\n",
        "        Initialize the Apriori algorithm for association rule mining.\n",
        "\n",
        "        Parameters:\n",
        "        - min_support: Minimum support threshold (0 to 1) for itemset frequency\n",
        "        - min_confidence: Minimum confidence threshold (0 to 1) for rule strength\n",
        "        \"\"\"\n",
        "        self.min_support = min_support\n",
        "        self.min_confidence = min_confidence\n",
        "        self.frequent_itemsets = None  # Dictionary storing frequent itemsets by size\n",
        "        self.association_rules = None  # List of generated association rules\n",
        "\n",
        "    def _calculate_support(self, itemset, transaction_records):\n",
        "        \"\"\"\n",
        "        Calculate the support value for an itemset.\n",
        "\n",
        "        Parameters:\n",
        "        - itemset: The itemset to evaluate\n",
        "        - transaction_records: List of all transactions\n",
        "\n",
        "        Returns:\n",
        "        - support: Fraction of transactions containing the itemset\n",
        "        \"\"\"\n",
        "        matching_transactions = sum(1 for transaction in transaction_records\n",
        "                                 if itemset.issubset(transaction))\n",
        "        return matching_transactions / len(transaction_records)\n",
        "\n",
        "    def _generate_candidate_itemsets(self, frequent_itemsets, itemset_size):\n",
        "        \"\"\"\n",
        "        Generate candidate itemsets of specified size using frequent itemsets.\n",
        "\n",
        "        Parameters:\n",
        "        - frequent_itemsets: Previously found frequent itemsets\n",
        "        - itemset_size: Desired size of new candidate itemsets\n",
        "\n",
        "        Returns:\n",
        "        - candidates: List of candidate itemsets\n",
        "        \"\"\"\n",
        "        unique_items = sorted({item for itemset in frequent_itemsets for item in itemset})\n",
        "        return [frozenset(combination) for combination in combinations(unique_items, itemset_size)]\n",
        "\n",
        "    def _prune_candidates(self, candidate_itemsets, previous_frequent_itemsets, itemset_size):\n",
        "        \"\"\"\n",
        "        Prune candidate itemsets that don't meet the Apriori property.\n",
        "\n",
        "        Parameters:\n",
        "        - candidate_itemsets: Generated candidate itemsets\n",
        "        - previous_frequent_itemsets: Frequent itemsets from previous iteration\n",
        "        - itemset_size: Size of current candidate itemsets\n",
        "\n",
        "        Returns:\n",
        "        - valid_candidates: Candidates that satisfy the Apriori property\n",
        "        \"\"\"\n",
        "        valid_candidates = []\n",
        "        previous_itemsets = set(previous_frequent_itemsets)\n",
        "\n",
        "        for candidate in candidate_itemsets:\n",
        "            # Check if all subsets of size k-1 are frequent\n",
        "            if all(frozenset(subset) in previous_itemsets\n",
        "                   for subset in combinations(candidate, itemset_size-1)):\n",
        "                valid_candidates.append(candidate)\n",
        "\n",
        "        return valid_candidates\n",
        "\n",
        "    def fit(self, transaction_records):\n",
        "        \"\"\"\n",
        "        Execute the Apriori algorithm to find frequent itemsets and association rules.\n",
        "\n",
        "        Parameters:\n",
        "        - transaction_records: List of transactions (each transaction is a list of items)\n",
        "        \"\"\"\n",
        "        # Convert transactions to frozensets for efficient subset operations\n",
        "        transaction_records = [frozenset(transaction) for transaction in transaction_records]\n",
        "\n",
        "        # Initialize storage for frequent itemsets\n",
        "        self.frequent_itemsets = {}\n",
        "        current_itemset_size = 1\n",
        "\n",
        "        # Start with single items\n",
        "        unique_items = {item for transaction in transaction_records for item in transaction}\n",
        "        candidate_itemsets = [frozenset([item]) for item in unique_items]\n",
        "\n",
        "        while candidate_itemsets:\n",
        "            # Evaluate support for current candidates\n",
        "            frequent_items = []\n",
        "            for itemset in candidate_itemsets:\n",
        "                support = self._calculate_support(itemset, transaction_records)\n",
        "                if support >= self.min_support:\n",
        "                    frequent_items.append((itemset, support))\n",
        "\n",
        "            # Store frequent itemsets of current size if any found\n",
        "            if frequent_items:\n",
        "                self.frequent_itemsets[current_itemset_size] = frequent_items\n",
        "                current_itemset_size += 1\n",
        "\n",
        "                # Generate next level candidates\n",
        "                candidate_itemsets = self._generate_candidate_itemsets(\n",
        "                    [itemset for itemset, _ in frequent_items],\n",
        "                    current_itemset_size\n",
        "                )\n",
        "\n",
        "                # Prune invalid candidates\n",
        "                frequent_itemsets_only = [itemset for itemset, _ in frequent_items]\n",
        "                candidate_itemsets = self._prune_candidates(\n",
        "                    candidate_itemsets,\n",
        "                    frequent_itemsets_only,\n",
        "                    current_itemset_size-1\n",
        "                )\n",
        "            else:\n",
        "                candidate_itemsets = None\n",
        "\n",
        "        # Generate association rules from frequent itemsets\n",
        "        self.association_rules = []\n",
        "        for itemset_size, itemsets in self.frequent_itemsets.items():\n",
        "            if itemset_size == 1:  # Skip single items\n",
        "                continue\n",
        "\n",
        "            for itemset, itemset_support in itemsets:\n",
        "                # Generate all possible non-empty subsets as antecedents\n",
        "                for antecedent_size in range(1, itemset_size):\n",
        "                    for antecedent in combinations(itemset, antecedent_size):\n",
        "                        antecedent = frozenset(antecedent)\n",
        "                        consequent = itemset - antecedent\n",
        "\n",
        "                        # Calculate rule confidence\n",
        "                        antecedent_support = self._calculate_support(antecedent, transaction_records)\n",
        "                        confidence = itemset_support / antecedent_support\n",
        "\n",
        "                        if confidence >= self.min_confidence:\n",
        "                            self.association_rules.append({\n",
        "                                'antecedent': antecedent,\n",
        "                                'consequent': consequent,\n",
        "                                'support': itemset_support,\n",
        "                                'confidence': confidence\n",
        "                            })\n",
        "\n",
        "    def get_frequent_itemsets(self, itemset_size=None):\n",
        "        \"\"\"\n",
        "        Retrieve frequent itemsets found by the algorithm.\n",
        "\n",
        "        Parameters:\n",
        "        - itemset_size: Specific size of itemsets to return (None returns all)\n",
        "\n",
        "        Returns:\n",
        "        - List of tuples containing (itemset, support) pairs\n",
        "        \"\"\"\n",
        "        if itemset_size is None:\n",
        "            return [itemset for size_group in self.frequent_itemsets.values()\n",
        "                   for itemset in size_group]\n",
        "        return self.frequent_itemsets.get(itemset_size, [])\n",
        "\n",
        "    def get_association_rules(self):\n",
        "        \"\"\"\n",
        "        Retrieve association rules meeting minimum confidence threshold.\n",
        "\n",
        "        Returns:\n",
        "        - List of dictionaries containing rule components and metrics\n",
        "        \"\"\"\n",
        "        return self.association_rules\n",
        "\n",
        "# Demonstration of Apriori algorithm\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample market basket transactions\n",
        "    market_baskets = [\n",
        "        ['bread', 'milk'],\n",
        "        ['bread', 'diapers', 'beer', 'eggs'],\n",
        "        ['milk', 'diapers', 'beer', 'cola'],\n",
        "        ['bread', 'milk', 'diapers', 'beer'],\n",
        "        ['bread', 'milk', 'diapers', 'cola']\n",
        "    ]\n",
        "\n",
        "    # Initialize and execute Apriori algorithm\n",
        "    apriori_analyzer = Apriori(min_support=0.4, min_confidence=0.6)\n",
        "    apriori_analyzer.fit(market_baskets)\n",
        "\n",
        "    # Display discovered frequent itemsets\n",
        "    print(\"Frequent Itemsets:\")\n",
        "    for size, itemsets in apriori_analyzer.frequent_itemsets.items():\n",
        "        print(f\"\\nItemset Size {size}:\")\n",
        "        for itemset, support in itemsets:\n",
        "            print(f\"{set(itemset)}: Support = {support:.2f}\")\n",
        "\n",
        "    # Display generated association rules\n",
        "    print(\"\\nAssociation Rules:\")\n",
        "    for rule in apriori_analyzer.get_association_rules():\n",
        "        print(f\"{set(rule['antecedent'])} => {set(rule['consequent'])} \"\n",
        "              f\"(Support={rule['support']:.2f}, Confidence={rule['confidence']:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuJbw2wSCoXu",
        "outputId": "c6586803-93f5-44f5-c59c-9990f006e987"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "\n",
            "Itemset Size 1:\n",
            "{'diapers'}: Support = 0.80\n",
            "{'bread'}: Support = 0.80\n",
            "{'beer'}: Support = 0.60\n",
            "{'cola'}: Support = 0.40\n",
            "{'milk'}: Support = 0.80\n",
            "\n",
            "Association Rules:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LfS0VI-3qiS3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}